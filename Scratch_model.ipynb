{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNVlqDTNsV3c2kggQuOEs/R"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **La class Couche**"
      ],
      "metadata": {
        "id": "q83_UYOE9IBz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "class Layer:\n",
        "    def __init__(self):\n",
        "        self.input = None\n",
        "        self.output = None\n",
        "\n",
        "    # Computes the output Y of a layer for a given input X\n",
        "    def forward_propagation(self, input_data):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    # Computes dE/dX for a given dE/dY (and updates parameters if any)\n",
        "    def backward_propagation(self, output_error, learning_rate):\n",
        "        raise NotImplementedError"
      ],
      "metadata": {
        "id": "vQ-7vre97YRa"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "53ogoC2r9Um9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "class FCLayer(Layer):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super().__init__()\n",
        "        # Initialize weights and biases randomly as told by our professor\n",
        "        self.weights = np.random.rand(input_size, output_size) - 0.5\n",
        "        self.bias = np.random.rand(1, output_size) - 0.5\n",
        "\n",
        "    def forward_propagation(self, input_data):\n",
        "        self.input = input_data\n",
        "        self.output = np.dot(self.input, self.weights) + self.bias\n",
        "        return self.output\n",
        "\n",
        "    def backward_propagation(self, output_error, learning_rate):\n",
        "        # Calculate gradient for weights and biases\n",
        "        input_error = np.dot(output_error, self.weights.T)\n",
        "        weights_error = np.dot(self.input.T, output_error)\n",
        "\n",
        "\n",
        "        self.weights -= learning_rate * weights_error\n",
        "\n",
        "        self.bias -= learning_rate * np.sum(output_error, axis=0, keepdims=True)\n",
        "\n",
        "        return input_error"
      ],
      "metadata": {
        "id": "AWfz2-Ok7bhG"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Activation**"
      ],
      "metadata": {
        "id": "WrYSPjXk9Y8n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class ActivationLayer(Layer):\n",
        "    def __init__(self, activation_function, activation_prime):\n",
        "        super().__init__()\n",
        "        self.activation = activation_function\n",
        "        self.activation_prime = activation_prime\n",
        "\n",
        "    def forward_propagation(self, input_data):\n",
        "        self.input = input_data\n",
        "        self.output = self.activation(self.input)\n",
        "        return self.output\n",
        "\n",
        "    def backward_propagation(self, output_error, learning_rate):\n",
        "\n",
        "        return output_error * self.activation_prime(self.input)"
      ],
      "metadata": {
        "id": "OSUiUVAD7eaM"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Activation and loss functions**"
      ],
      "metadata": {
        "id": "HnlAufZk9i7h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_prime(x):\n",
        "    return sigmoid(x) * (1 - sigmoid(x))\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_prime(x):\n",
        "    return (x > 0).astype(float)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def mse(y_true, y_pred):\n",
        "    return np.mean(np.power(y_true - y_pred, 2))\n",
        "\n",
        "# Derive du Mse\n",
        "def mse_prime(y_true, y_pred):\n",
        "    return 2 * (y_pred - y_true) / y_true.size\n"
      ],
      "metadata": {
        "id": "PPTOIzUr7fpA"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Network class with Add predict train .....**"
      ],
      "metadata": {
        "id": "OBgRdKp19saz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# n.n.m\n",
        "class Network:\n",
        "    def __init__(self):\n",
        "        self.layers = []\n",
        "\n",
        "    def add(self, layer):\n",
        "        self.layers.append(layer)\n",
        "\n",
        "    def predict(self, input_data):\n",
        "        output = input_data\n",
        "        for layer in self.layers:\n",
        "            output = layer.forward_propagation(output)\n",
        "        return output\n",
        "\n",
        "    def train(self, X_train, y_train, epochs, learning_rate):\n",
        "        for i in range(epochs):\n",
        "            output = self.predict(X_train)\n",
        "            error = mse_prime(y_train, output)\n",
        "\n",
        "            # Backward propagation\n",
        "            for layer in reversed(self.layers):\n",
        "                error = layer.backward_propagation(error, learning_rate)\n",
        "\n",
        "            # printi loss f kola epoch\n",
        "            if i % 100 == 0:\n",
        "                loss = mse(y_train, self.predict(X_train))\n",
        "                print(f\"Epoch {i}, Loss: {loss:.4f}\")"
      ],
      "metadata": {
        "id": "FNzCDyIv7xy-"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Main to test**"
      ],
      "metadata": {
        "id": "PYJ2WxCV91jG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    X = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
        "    y = np.array([[0], [1], [1], [0]])\n",
        "\n",
        "    nn = Network()\n",
        "    nn.add(FCLayer(2, 4))\n",
        "    nn.add(ActivationLayer(sigmoid, sigmoid_prime))\n",
        "    nn.add(FCLayer(4, 1))\n",
        "    nn.add(ActivationLayer(sigmoid, sigmoid_prime))\n",
        "\n",
        "    print(\"--- Training ---\")\n",
        "    nn.train(X, y, epochs=10000, learning_rate=0.1)\n",
        "    print(\"\\n--- Predictions after training ---\")\n",
        "    predictions = nn.predict(X)\n",
        "    print(predictions.round()) # Round to 0 or 1 for classification"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVSM-gOe7Ode",
        "outputId": "c62f2edb-eafd-4608-aedf-3dceebfff54b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Training ---\n",
            "Epoch 0, Loss: 0.2736\n",
            "Epoch 100, Loss: 0.2502\n",
            "Epoch 200, Loss: 0.2500\n",
            "Epoch 300, Loss: 0.2500\n",
            "Epoch 400, Loss: 0.2500\n",
            "Epoch 500, Loss: 0.2500\n",
            "Epoch 600, Loss: 0.2500\n",
            "Epoch 700, Loss: 0.2500\n",
            "Epoch 800, Loss: 0.2500\n",
            "Epoch 900, Loss: 0.2500\n",
            "Epoch 1000, Loss: 0.2500\n",
            "Epoch 1100, Loss: 0.2500\n",
            "Epoch 1200, Loss: 0.2500\n",
            "Epoch 1300, Loss: 0.2500\n",
            "Epoch 1400, Loss: 0.2500\n",
            "Epoch 1500, Loss: 0.2500\n",
            "Epoch 1600, Loss: 0.2500\n",
            "Epoch 1700, Loss: 0.2500\n",
            "Epoch 1800, Loss: 0.2500\n",
            "Epoch 1900, Loss: 0.2500\n",
            "Epoch 2000, Loss: 0.2500\n",
            "Epoch 2100, Loss: 0.2500\n",
            "Epoch 2200, Loss: 0.2500\n",
            "Epoch 2300, Loss: 0.2500\n",
            "Epoch 2400, Loss: 0.2500\n",
            "Epoch 2500, Loss: 0.2500\n",
            "Epoch 2600, Loss: 0.2500\n",
            "Epoch 2700, Loss: 0.2500\n",
            "Epoch 2800, Loss: 0.2500\n",
            "Epoch 2900, Loss: 0.2500\n",
            "Epoch 3000, Loss: 0.2500\n",
            "Epoch 3100, Loss: 0.2500\n",
            "Epoch 3200, Loss: 0.2500\n",
            "Epoch 3300, Loss: 0.2500\n",
            "Epoch 3400, Loss: 0.2500\n",
            "Epoch 3500, Loss: 0.2500\n",
            "Epoch 3600, Loss: 0.2500\n",
            "Epoch 3700, Loss: 0.2500\n",
            "Epoch 3800, Loss: 0.2500\n",
            "Epoch 3900, Loss: 0.2500\n",
            "Epoch 4000, Loss: 0.2499\n",
            "Epoch 4100, Loss: 0.2499\n",
            "Epoch 4200, Loss: 0.2499\n",
            "Epoch 4300, Loss: 0.2499\n",
            "Epoch 4400, Loss: 0.2499\n",
            "Epoch 4500, Loss: 0.2499\n",
            "Epoch 4600, Loss: 0.2499\n",
            "Epoch 4700, Loss: 0.2499\n",
            "Epoch 4800, Loss: 0.2499\n",
            "Epoch 4900, Loss: 0.2499\n",
            "Epoch 5000, Loss: 0.2499\n",
            "Epoch 5100, Loss: 0.2499\n",
            "Epoch 5200, Loss: 0.2499\n",
            "Epoch 5300, Loss: 0.2499\n",
            "Epoch 5400, Loss: 0.2499\n",
            "Epoch 5500, Loss: 0.2499\n",
            "Epoch 5600, Loss: 0.2499\n",
            "Epoch 5700, Loss: 0.2499\n",
            "Epoch 5800, Loss: 0.2499\n",
            "Epoch 5900, Loss: 0.2499\n",
            "Epoch 6000, Loss: 0.2499\n",
            "Epoch 6100, Loss: 0.2499\n",
            "Epoch 6200, Loss: 0.2499\n",
            "Epoch 6300, Loss: 0.2499\n",
            "Epoch 6400, Loss: 0.2499\n",
            "Epoch 6500, Loss: 0.2499\n",
            "Epoch 6600, Loss: 0.2499\n",
            "Epoch 6700, Loss: 0.2499\n",
            "Epoch 6800, Loss: 0.2499\n",
            "Epoch 6900, Loss: 0.2498\n",
            "Epoch 7000, Loss: 0.2498\n",
            "Epoch 7100, Loss: 0.2498\n",
            "Epoch 7200, Loss: 0.2498\n",
            "Epoch 7300, Loss: 0.2498\n",
            "Epoch 7400, Loss: 0.2498\n",
            "Epoch 7500, Loss: 0.2498\n",
            "Epoch 7600, Loss: 0.2498\n",
            "Epoch 7700, Loss: 0.2498\n",
            "Epoch 7800, Loss: 0.2498\n",
            "Epoch 7900, Loss: 0.2498\n",
            "Epoch 8000, Loss: 0.2498\n",
            "Epoch 8100, Loss: 0.2497\n",
            "Epoch 8200, Loss: 0.2497\n",
            "Epoch 8300, Loss: 0.2497\n",
            "Epoch 8400, Loss: 0.2497\n",
            "Epoch 8500, Loss: 0.2497\n",
            "Epoch 8600, Loss: 0.2497\n",
            "Epoch 8700, Loss: 0.2497\n",
            "Epoch 8800, Loss: 0.2496\n",
            "Epoch 8900, Loss: 0.2496\n",
            "Epoch 9000, Loss: 0.2496\n",
            "Epoch 9100, Loss: 0.2496\n",
            "Epoch 9200, Loss: 0.2495\n",
            "Epoch 9300, Loss: 0.2495\n",
            "Epoch 9400, Loss: 0.2495\n",
            "Epoch 9500, Loss: 0.2494\n",
            "Epoch 9600, Loss: 0.2494\n",
            "Epoch 9700, Loss: 0.2494\n",
            "Epoch 9800, Loss: 0.2493\n",
            "Epoch 9900, Loss: 0.2493\n",
            "\n",
            "--- Predictions after training ---\n",
            "[[1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]]\n"
          ]
        }
      ]
    }
  ]
}